{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train.csv', parse_dates=['pickup_datetime', 'dropoff_datetime'])\n",
    "test_data = pd.read_csv('test.csv', parse_dates=['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1458644 entries, 0 to 1458643\n",
      "Data columns (total 11 columns):\n",
      "id                    1458644 non-null object\n",
      "vendor_id             1458644 non-null int64\n",
      "pickup_datetime       1458644 non-null datetime64[ns]\n",
      "dropoff_datetime      1458644 non-null datetime64[ns]\n",
      "passenger_count       1458644 non-null int64\n",
      "pickup_longitude      1458644 non-null float64\n",
      "pickup_latitude       1458644 non-null float64\n",
      "dropoff_longitude     1458644 non-null float64\n",
      "dropoff_latitude      1458644 non-null float64\n",
      "store_and_fwd_flag    1458644 non-null object\n",
      "trip_duration         1458644 non-null int64\n",
      "dtypes: datetime64[ns](2), float64(4), int64(3), object(2)\n",
      "memory usage: 122.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tocluster = train_data[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = []\n",
    "#for k in range(3, 16):\n",
    "#    print(\"Current K is\", k)\n",
    "#    kmeans = KMeans(n_clusters=k, random_state=10)\n",
    "#    labels = kmeans.fit(tocluster).labels_\n",
    "#    scores.append(silhouette_score(tocluster, labels, metric='euclidean', sample_size=1000))\n",
    "#plt.plot(range(3,16), scores)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kmeans = KMeans(n_clusters=8, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cluster_labels = kmeans.fit(tocluster).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some useful features from existing ones. At least, the following will be preferable:\n",
    "1) distance between start and end points of the trip;\n",
    "2) week of the day\n",
    "3) month number\n",
    "4) second of the current day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def distance_on_sphere(lat1, long1, lat2, long2):\n",
    "    d2r = np.pi / 180.0\n",
    "    phi1 = (90.0 - lat1) * d2r\n",
    "    phi2 = (90.0 - lat2) * d2r\n",
    "    # theta = долгота\n",
    "    theta1 = long1 * d2r\n",
    "    theta2 = long2 * d2r\n",
    "    carc = (np.sin(phi1) * np.sin(phi2) * np.cos(theta1 - theta2) +\n",
    "           np.cos(phi1) * np.cos(phi2))\n",
    "    if np.abs(carc) > 1.0:\n",
    "        return np.arccos(np.sign(carc) * 1.0)\n",
    "    return np.arccos(carc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X):\n",
    "    res = X.copy()\n",
    "    distances = 6371 * distance_on_sphere(X[['pickup_latitude']].values,\n",
    "                               X[['pickup_longitude']].values,\n",
    "                               X[['dropoff_latitude']].values,\n",
    "                               X[['dropoff_longitude']].values)\n",
    "\n",
    "    res['dist'] = distances\n",
    "\n",
    "    month = X['pickup_datetime'].map(lambda x: x.month)\n",
    "    res['month'] = (month - month.min()) / (month.max() - month.min())\n",
    "    \n",
    "    wd = X['pickup_datetime'].map(lambda x: x.weekday())\n",
    "    res['wdsin'] = np.sin(2 * np.pi * wd / 7.0)\n",
    "    res['wdcos'] = np.cos(2 * np.pi * wd / 7.0)\n",
    "    \n",
    "    sc = X['pickup_datetime'].map(lambda x: x.hour * 3600.0 + x.minute * 60.0 + x.second)\n",
    "    res['scsin'] = np.sin(2 * np.pi * sc / 86400.0)\n",
    "    res['sccos'] = np.cos(2 * np.pi * sc / 86400.0)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Detect and drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_train_dataset(X): \n",
    "    strange_records1 = (X['dist'] < 0.1) & (X['trip_duration'] > 5000)\n",
    "    strange_records2 = (X['dist'] < 100) & (X['trip_duration'] > 80000)\n",
    "    return X.loc[~(strange_records1|strange_records2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_analyze_filtered = filter_train_dataset(get_features(train_data))\n",
    "y = to_analyze_filtered[['trip_duration']].values\n",
    "X = to_analyze_filtered.drop(['store_and_fwd_flag','vendor_id', 'id', 'pickup_datetime', 'dropoff_datetime','trip_duration'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterized_regressor(traindataset, testdataset=None):\n",
    "    to_analyze_filtered = filter_train_dataset(get_features(traindataset))\n",
    "    tocluster = to_analyze_filtered[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].values\n",
    "    kmeans = KMeans(n_clusters=8, random_state=10, n_jobs=-1)\n",
    "    cluster_labels = kmeans.fit(tocluster).labels_\n",
    "    clfs = {}\n",
    "    y = to_analyze_filtered[['trip_duration']].values\n",
    "    X = to_analyze_filtered.drop(['store_and_fwd_flag','vendor_id', 'id', 'pickup_datetime', 'dropoff_datetime','trip_duration'], axis=1).values\n",
    "    for lb in np.unique(cluster_labels):\n",
    "        if sum(cluster_labels==lb):\n",
    "            clf = RandomForestRegressor(max_depth=5, random_state=0, n_jobs=-1, n_estimators=10)\n",
    "            _X, _y = X[cluster_labels==lb,:], y[cluster_labels==lb]\n",
    "            clf.fit(_X, _y.ravel())\n",
    "            clfs.update({lb: clf.predict})\n",
    "        else:\n",
    "            clfs.update({lb: lambda x: to_analyze_filtered.loc[cluster_labels==lb, 'trip_duration'].median()})\n",
    "        print(\"Evaluating the lb: \", lb)\n",
    "    if testdataset is not None:\n",
    "        to_analyze_test = get_features(testdataset)\n",
    "        tocluster = testdataset[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].values\n",
    "        test_labels = kmeans.predict(tocluster)\n",
    "        X = to_analyze_test.drop(['store_and_fwd_flag', 'vendor_id', 'id', 'pickup_datetime'], axis=1).values\n",
    "        predictions = []\n",
    "        for lb, data in zip(test_labels, X):\n",
    "            value = clfs[lb]([data])\n",
    "            predictions.append(value)\n",
    "        to_analyze_test['pred'] = predictions\n",
    "        return to_analyze_test\n",
    "    else:\n",
    "        return clfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the lb:  0\n",
      "Evaluating the lb:  1\n",
      "Evaluating the lb:  2\n",
      "Evaluating the lb:  3\n",
      "Evaluating the lb:  4\n",
      "Evaluating the lb:  5\n",
      "Evaluating the lb:  6\n",
      "Evaluating the lb:  7\n"
     ]
    }
   ],
   "source": [
    "result = clusterized_regressor(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
